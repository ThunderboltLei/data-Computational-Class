<!DOCTYPE html>
<!-- saved from url=(0089)https://towardsdatascience.com/media/8f328e36827c11e97dfaeb217f285b66?postId=81fc5f8c4e8e -->
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><title>torch101_updating_parms.py – Medium</title><meta name="description" content="GitHub Gist: instantly share code, notes, and snippets."><meta name="twitter:widgets:csp" content="on"><meta name="robots" content="noindex"><!--<base target="_blank">--><base href="." target="_blank"><style>body {text-rendering: optimizeLegibility; -webkit-font-smoothing: antialiased; -moz-osx-font-smoothing: grayscale; font-family: "ff-tisa-web-pro", Georgia, Cambria, "Times New Roman", Times, serif; font-weight: 400; color: #333332; font-size: 18px; line-height: 1.4; margin: 0; background-color: white; overflow: hidden;}iframe {max-width: 100%;}</style></head><body><style>.gist .gist-file { margin-bottom: 0 !important; }.gist { text-rendering: auto; }</style><script src="./14581c746d2c57404f8ccfaecfe829df.js" charset="utf-8"></script><link rel="stylesheet" href="./gist-embed-a9a1cf2ca01efd362bfa52312712ae94.css"><div id="gist95929004" class="gist">
    <div class="gist-file">
      <div class="gist-data">
        <div class="js-gist-file-update-container js-task-list-container file-box">
  <div id="file-torch101_updating_parms-py" class="file">
    

  <div itemprop="text" class="Box-body p-0 blob-wrapper data type-python ">
      
<table class="highlight tab-size js-file-line-container" data-tab-size="8">
      <tbody><tr>
        <td id="file-torch101_updating_parms-py-L1" class="blob-num js-line-number" data-line-number="1"></td>
        <td id="file-torch101_updating_parms-py-LC1" class="blob-code blob-code-inner js-file-line">lr <span class="pl-k">=</span> <span class="pl-c1">1e-1</span></td>
      </tr>
      <tr>
        <td id="file-torch101_updating_parms-py-L2" class="blob-num js-line-number" data-line-number="2"></td>
        <td id="file-torch101_updating_parms-py-LC2" class="blob-code blob-code-inner js-file-line">n_epochs <span class="pl-k">=</span> <span class="pl-c1">1000</span></td>
      </tr>
      <tr>
        <td id="file-torch101_updating_parms-py-L3" class="blob-num js-line-number" data-line-number="3"></td>
        <td id="file-torch101_updating_parms-py-LC3" class="blob-code blob-code-inner js-file-line">
</td>
      </tr>
      <tr>
        <td id="file-torch101_updating_parms-py-L4" class="blob-num js-line-number" data-line-number="4"></td>
        <td id="file-torch101_updating_parms-py-LC4" class="blob-code blob-code-inner js-file-line">torch.manual_seed(<span class="pl-c1">42</span>)</td>
      </tr>
      <tr>
        <td id="file-torch101_updating_parms-py-L5" class="blob-num js-line-number" data-line-number="5"></td>
        <td id="file-torch101_updating_parms-py-LC5" class="blob-code blob-code-inner js-file-line">a <span class="pl-k">=</span> torch.randn(<span class="pl-c1">1</span>, <span class="pl-v">requires_grad</span><span class="pl-k">=</span><span class="pl-c1">True</span>, <span class="pl-v">dtype</span><span class="pl-k">=</span>torch.float, <span class="pl-v">device</span><span class="pl-k">=</span>device)</td>
      </tr>
      <tr>
        <td id="file-torch101_updating_parms-py-L6" class="blob-num js-line-number" data-line-number="6"></td>
        <td id="file-torch101_updating_parms-py-LC6" class="blob-code blob-code-inner js-file-line">b <span class="pl-k">=</span> torch.randn(<span class="pl-c1">1</span>, <span class="pl-v">requires_grad</span><span class="pl-k">=</span><span class="pl-c1">True</span>, <span class="pl-v">dtype</span><span class="pl-k">=</span>torch.float, <span class="pl-v">device</span><span class="pl-k">=</span>device)</td>
      </tr>
      <tr>
        <td id="file-torch101_updating_parms-py-L7" class="blob-num js-line-number" data-line-number="7"></td>
        <td id="file-torch101_updating_parms-py-LC7" class="blob-code blob-code-inner js-file-line">
</td>
      </tr>
      <tr>
        <td id="file-torch101_updating_parms-py-L8" class="blob-num js-line-number" data-line-number="8"></td>
        <td id="file-torch101_updating_parms-py-LC8" class="blob-code blob-code-inner js-file-line"><span class="pl-k">for</span> epoch <span class="pl-k">in</span> <span class="pl-c1">range</span>(n_epochs):</td>
      </tr>
      <tr>
        <td id="file-torch101_updating_parms-py-L9" class="blob-num js-line-number" data-line-number="9"></td>
        <td id="file-torch101_updating_parms-py-LC9" class="blob-code blob-code-inner js-file-line">    yhat <span class="pl-k">=</span> a <span class="pl-k">+</span> b <span class="pl-k">*</span> x_train_tensor</td>
      </tr>
      <tr>
        <td id="file-torch101_updating_parms-py-L10" class="blob-num js-line-number" data-line-number="10"></td>
        <td id="file-torch101_updating_parms-py-LC10" class="blob-code blob-code-inner js-file-line">    error <span class="pl-k">=</span> y_train_tensor <span class="pl-k">-</span> yhat</td>
      </tr>
      <tr>
        <td id="file-torch101_updating_parms-py-L11" class="blob-num js-line-number" data-line-number="11"></td>
        <td id="file-torch101_updating_parms-py-LC11" class="blob-code blob-code-inner js-file-line">    loss <span class="pl-k">=</span> (error <span class="pl-k">**</span> <span class="pl-c1">2</span>).mean()</td>
      </tr>
      <tr>
        <td id="file-torch101_updating_parms-py-L12" class="blob-num js-line-number" data-line-number="12"></td>
        <td id="file-torch101_updating_parms-py-LC12" class="blob-code blob-code-inner js-file-line">
</td>
      </tr>
      <tr>
        <td id="file-torch101_updating_parms-py-L13" class="blob-num js-line-number" data-line-number="13"></td>
        <td id="file-torch101_updating_parms-py-LC13" class="blob-code blob-code-inner js-file-line">    <span class="pl-c"><span class="pl-c">#</span> No more manual computation of gradients! </span></td>
      </tr>
      <tr>
        <td id="file-torch101_updating_parms-py-L14" class="blob-num js-line-number" data-line-number="14"></td>
        <td id="file-torch101_updating_parms-py-LC14" class="blob-code blob-code-inner js-file-line">    <span class="pl-c"><span class="pl-c">#</span> a_grad = -2 * error.mean()</span></td>
      </tr>
      <tr>
        <td id="file-torch101_updating_parms-py-L15" class="blob-num js-line-number" data-line-number="15"></td>
        <td id="file-torch101_updating_parms-py-LC15" class="blob-code blob-code-inner js-file-line">    <span class="pl-c"><span class="pl-c">#</span> b_grad = -2 * (x_tensor * error).mean()</span></td>
      </tr>
      <tr>
        <td id="file-torch101_updating_parms-py-L16" class="blob-num js-line-number" data-line-number="16"></td>
        <td id="file-torch101_updating_parms-py-LC16" class="blob-code blob-code-inner js-file-line">    </td>
      </tr>
      <tr>
        <td id="file-torch101_updating_parms-py-L17" class="blob-num js-line-number" data-line-number="17"></td>
        <td id="file-torch101_updating_parms-py-LC17" class="blob-code blob-code-inner js-file-line">    <span class="pl-c"><span class="pl-c">#</span> We just tell PyTorch to work its way BACKWARDS from the specified loss!</span></td>
      </tr>
      <tr>
        <td id="file-torch101_updating_parms-py-L18" class="blob-num js-line-number" data-line-number="18"></td>
        <td id="file-torch101_updating_parms-py-LC18" class="blob-code blob-code-inner js-file-line">    loss.backward()</td>
      </tr>
      <tr>
        <td id="file-torch101_updating_parms-py-L19" class="blob-num js-line-number" data-line-number="19"></td>
        <td id="file-torch101_updating_parms-py-LC19" class="blob-code blob-code-inner js-file-line">    <span class="pl-c"><span class="pl-c">#</span> Let's check the computed gradients...</span></td>
      </tr>
      <tr>
        <td id="file-torch101_updating_parms-py-L20" class="blob-num js-line-number" data-line-number="20"></td>
        <td id="file-torch101_updating_parms-py-LC20" class="blob-code blob-code-inner js-file-line">    <span class="pl-c1">print</span>(a.grad)</td>
      </tr>
      <tr>
        <td id="file-torch101_updating_parms-py-L21" class="blob-num js-line-number" data-line-number="21"></td>
        <td id="file-torch101_updating_parms-py-LC21" class="blob-code blob-code-inner js-file-line">    <span class="pl-c1">print</span>(b.grad)</td>
      </tr>
      <tr>
        <td id="file-torch101_updating_parms-py-L22" class="blob-num js-line-number" data-line-number="22"></td>
        <td id="file-torch101_updating_parms-py-LC22" class="blob-code blob-code-inner js-file-line">    </td>
      </tr>
      <tr>
        <td id="file-torch101_updating_parms-py-L23" class="blob-num js-line-number" data-line-number="23"></td>
        <td id="file-torch101_updating_parms-py-LC23" class="blob-code blob-code-inner js-file-line">    <span class="pl-c"><span class="pl-c">#</span> What about UPDATING the parameters? Not so fast...</span></td>
      </tr>
      <tr>
        <td id="file-torch101_updating_parms-py-L24" class="blob-num js-line-number" data-line-number="24"></td>
        <td id="file-torch101_updating_parms-py-LC24" class="blob-code blob-code-inner js-file-line">    </td>
      </tr>
      <tr>
        <td id="file-torch101_updating_parms-py-L25" class="blob-num js-line-number" data-line-number="25"></td>
        <td id="file-torch101_updating_parms-py-LC25" class="blob-code blob-code-inner js-file-line">    <span class="pl-c"><span class="pl-c">#</span> FIRST ATTEMPT</span></td>
      </tr>
      <tr>
        <td id="file-torch101_updating_parms-py-L26" class="blob-num js-line-number" data-line-number="26"></td>
        <td id="file-torch101_updating_parms-py-LC26" class="blob-code blob-code-inner js-file-line">    <span class="pl-c"><span class="pl-c">#</span> AttributeError: 'NoneType' object has no attribute 'zero_'</span></td>
      </tr>
      <tr>
        <td id="file-torch101_updating_parms-py-L27" class="blob-num js-line-number" data-line-number="27"></td>
        <td id="file-torch101_updating_parms-py-LC27" class="blob-code blob-code-inner js-file-line">    <span class="pl-c"><span class="pl-c">#</span> a = a - lr * a.grad</span></td>
      </tr>
      <tr>
        <td id="file-torch101_updating_parms-py-L28" class="blob-num js-line-number" data-line-number="28"></td>
        <td id="file-torch101_updating_parms-py-LC28" class="blob-code blob-code-inner js-file-line">    <span class="pl-c"><span class="pl-c">#</span> b = b - lr * b.grad</span></td>
      </tr>
      <tr>
        <td id="file-torch101_updating_parms-py-L29" class="blob-num js-line-number" data-line-number="29"></td>
        <td id="file-torch101_updating_parms-py-LC29" class="blob-code blob-code-inner js-file-line">    <span class="pl-c"><span class="pl-c">#</span> print(a)</span></td>
      </tr>
      <tr>
        <td id="file-torch101_updating_parms-py-L30" class="blob-num js-line-number" data-line-number="30"></td>
        <td id="file-torch101_updating_parms-py-LC30" class="blob-code blob-code-inner js-file-line">
</td>
      </tr>
      <tr>
        <td id="file-torch101_updating_parms-py-L31" class="blob-num js-line-number" data-line-number="31"></td>
        <td id="file-torch101_updating_parms-py-LC31" class="blob-code blob-code-inner js-file-line">    <span class="pl-c"><span class="pl-c">#</span> SECOND ATTEMPT</span></td>
      </tr>
      <tr>
        <td id="file-torch101_updating_parms-py-L32" class="blob-num js-line-number" data-line-number="32"></td>
        <td id="file-torch101_updating_parms-py-LC32" class="blob-code blob-code-inner js-file-line">    <span class="pl-c"><span class="pl-c">#</span> RuntimeError: a leaf Variable that requires grad has been used in an in-place operation.</span></td>
      </tr>
      <tr>
        <td id="file-torch101_updating_parms-py-L33" class="blob-num js-line-number" data-line-number="33"></td>
        <td id="file-torch101_updating_parms-py-LC33" class="blob-code blob-code-inner js-file-line">    <span class="pl-c"><span class="pl-c">#</span> a -= lr * a.grad</span></td>
      </tr>
      <tr>
        <td id="file-torch101_updating_parms-py-L34" class="blob-num js-line-number" data-line-number="34"></td>
        <td id="file-torch101_updating_parms-py-LC34" class="blob-code blob-code-inner js-file-line">    <span class="pl-c"><span class="pl-c">#</span> b -= lr * b.grad        </span></td>
      </tr>
      <tr>
        <td id="file-torch101_updating_parms-py-L35" class="blob-num js-line-number" data-line-number="35"></td>
        <td id="file-torch101_updating_parms-py-LC35" class="blob-code blob-code-inner js-file-line">    </td>
      </tr>
      <tr>
        <td id="file-torch101_updating_parms-py-L36" class="blob-num js-line-number" data-line-number="36"></td>
        <td id="file-torch101_updating_parms-py-LC36" class="blob-code blob-code-inner js-file-line">    <span class="pl-c"><span class="pl-c">#</span> THIRD ATTEMPT</span></td>
      </tr>
      <tr>
        <td id="file-torch101_updating_parms-py-L37" class="blob-num js-line-number" data-line-number="37"></td>
        <td id="file-torch101_updating_parms-py-LC37" class="blob-code blob-code-inner js-file-line">    <span class="pl-c"><span class="pl-c">#</span> We need to use NO_GRAD to keep the update out of the gradient computation</span></td>
      </tr>
      <tr>
        <td id="file-torch101_updating_parms-py-L38" class="blob-num js-line-number" data-line-number="38"></td>
        <td id="file-torch101_updating_parms-py-LC38" class="blob-code blob-code-inner js-file-line">    <span class="pl-c"><span class="pl-c">#</span> Why is that? It boils down to the DYNAMIC GRAPH that PyTorch uses...</span></td>
      </tr>
      <tr>
        <td id="file-torch101_updating_parms-py-L39" class="blob-num js-line-number" data-line-number="39"></td>
        <td id="file-torch101_updating_parms-py-LC39" class="blob-code blob-code-inner js-file-line">    <span class="pl-k">with</span> torch.no_grad():</td>
      </tr>
      <tr>
        <td id="file-torch101_updating_parms-py-L40" class="blob-num js-line-number" data-line-number="40"></td>
        <td id="file-torch101_updating_parms-py-LC40" class="blob-code blob-code-inner js-file-line">        a <span class="pl-k">-=</span> lr <span class="pl-k">*</span> a.grad</td>
      </tr>
      <tr>
        <td id="file-torch101_updating_parms-py-L41" class="blob-num js-line-number" data-line-number="41"></td>
        <td id="file-torch101_updating_parms-py-LC41" class="blob-code blob-code-inner js-file-line">        b <span class="pl-k">-=</span> lr <span class="pl-k">*</span> b.grad</td>
      </tr>
      <tr>
        <td id="file-torch101_updating_parms-py-L42" class="blob-num js-line-number" data-line-number="42"></td>
        <td id="file-torch101_updating_parms-py-LC42" class="blob-code blob-code-inner js-file-line">    </td>
      </tr>
      <tr>
        <td id="file-torch101_updating_parms-py-L43" class="blob-num js-line-number" data-line-number="43"></td>
        <td id="file-torch101_updating_parms-py-LC43" class="blob-code blob-code-inner js-file-line">    <span class="pl-c"><span class="pl-c">#</span> PyTorch is "clingy" to its computed gradients, we need to tell it to let it go...</span></td>
      </tr>
      <tr>
        <td id="file-torch101_updating_parms-py-L44" class="blob-num js-line-number" data-line-number="44"></td>
        <td id="file-torch101_updating_parms-py-LC44" class="blob-code blob-code-inner js-file-line">    a.grad.zero_()</td>
      </tr>
      <tr>
        <td id="file-torch101_updating_parms-py-L45" class="blob-num js-line-number" data-line-number="45"></td>
        <td id="file-torch101_updating_parms-py-LC45" class="blob-code blob-code-inner js-file-line">    b.grad.zero_()</td>
      </tr>
      <tr>
        <td id="file-torch101_updating_parms-py-L46" class="blob-num js-line-number" data-line-number="46"></td>
        <td id="file-torch101_updating_parms-py-LC46" class="blob-code blob-code-inner js-file-line">    </td>
      </tr>
      <tr>
        <td id="file-torch101_updating_parms-py-L47" class="blob-num js-line-number" data-line-number="47"></td>
        <td id="file-torch101_updating_parms-py-LC47" class="blob-code blob-code-inner js-file-line"><span class="pl-c1">print</span>(a, b)</td>
      </tr>
</tbody></table>


  </div>

  </div>
</div>

      </div>
      <div class="gist-meta">
        <a href="https://gist.github.com/dvgodoy/14581c746d2c57404f8ccfaecfe829df/raw/b07498b13f19fa9377e2207a04ca321656a65a95/torch101_updating_parms.py" style="float:right">view raw</a>
        <a href="https://gist.github.com/dvgodoy/14581c746d2c57404f8ccfaecfe829df#file-torch101_updating_parms-py">torch101_updating_parms.py</a>
        hosted with ❤ by <a href="https://github.com/">GitHub</a>
      </div>
    </div>
</div>
<script>var height = -1; var delayMs = 200;function notifyResize(height) {height = height ? height : document.documentElement.offsetHeight; var resized = false; if (window.donkey && donkey.resize) {donkey.resize(height); resized = true;}if (parent && parent._resizeIframe) {var obj = {iframe: window.frameElement, height: height}; parent._resizeIframe(obj); resized = true;}if (window.location && window.location.hash === "#amp=1" && window.parent && window.parent.postMessage) {window.parent.postMessage({sentinel: "amp", type: "embed-size", height: height}, "*");}if (window.webkit && window.webkit.messageHandlers && window.webkit.messageHandlers.resize) {window.webkit.messageHandlers.resize.postMessage(height); resized = true;}return resized;}function maybeResize() {if (document.documentElement.offsetHeight != height && notifyResize()) {height = document.documentElement.offsetHeight;}delayMs = Math.min(delayMs * 2, 1000000); setTimeout(maybeResize, delayMs);}maybeResize();</script></body></html>