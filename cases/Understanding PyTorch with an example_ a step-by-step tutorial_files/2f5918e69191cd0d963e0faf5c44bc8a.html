<!DOCTYPE html>
<!-- saved from url=(0089)https://towardsdatascience.com/media/2f5918e69191cd0d963e0faf5c44bc8a?postId=81fc5f8c4e8e -->
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><title>torch101_bad_inits.py – Medium</title><meta name="description" content="GitHub Gist: instantly share code, notes, and snippets."><meta name="twitter:widgets:csp" content="on"><meta name="robots" content="noindex"><!--<base target="_blank">--><base href="." target="_blank"><style>body {text-rendering: optimizeLegibility; -webkit-font-smoothing: antialiased; -moz-osx-font-smoothing: grayscale; font-family: "ff-tisa-web-pro", Georgia, Cambria, "Times New Roman", Times, serif; font-weight: 400; color: #333332; font-size: 18px; line-height: 1.4; margin: 0; background-color: white; overflow: hidden;}iframe {max-width: 100%;}</style></head><body><style>.gist .gist-file { margin-bottom: 0 !important; }.gist { text-rendering: auto; }</style><script src="./a9b8521aa491ddc26105c6d655d2c933.js" charset="utf-8"></script><link rel="stylesheet" href="./gist-embed-a9a1cf2ca01efd362bfa52312712ae94.css"><div id="gist95928835" class="gist">
    <div class="gist-file">
      <div class="gist-data">
        <div class="js-gist-file-update-container js-task-list-container file-box">
  <div id="file-torch101_bad_inits-py" class="file">
    

  <div itemprop="text" class="Box-body p-0 blob-wrapper data type-python ">
      
<table class="highlight tab-size js-file-line-container" data-tab-size="8">
      <tbody><tr>
        <td id="file-torch101_bad_inits-py-L1" class="blob-num js-line-number" data-line-number="1"></td>
        <td id="file-torch101_bad_inits-py-LC1" class="blob-code blob-code-inner js-file-line"><span class="pl-c"><span class="pl-c">#</span> FIRST</span></td>
      </tr>
      <tr>
        <td id="file-torch101_bad_inits-py-L2" class="blob-num js-line-number" data-line-number="2"></td>
        <td id="file-torch101_bad_inits-py-LC2" class="blob-code blob-code-inner js-file-line"><span class="pl-c"><span class="pl-c">#</span> Initializes parameters "a" and "b" randomly, ALMOST as we did in Numpy</span></td>
      </tr>
      <tr>
        <td id="file-torch101_bad_inits-py-L3" class="blob-num js-line-number" data-line-number="3"></td>
        <td id="file-torch101_bad_inits-py-LC3" class="blob-code blob-code-inner js-file-line"><span class="pl-c"><span class="pl-c">#</span> since we want to apply gradient descent on these parameters, we need</span></td>
      </tr>
      <tr>
        <td id="file-torch101_bad_inits-py-L4" class="blob-num js-line-number" data-line-number="4"></td>
        <td id="file-torch101_bad_inits-py-LC4" class="blob-code blob-code-inner js-file-line"><span class="pl-c"><span class="pl-c">#</span> to set REQUIRES_GRAD = TRUE</span></td>
      </tr>
      <tr>
        <td id="file-torch101_bad_inits-py-L5" class="blob-num js-line-number" data-line-number="5"></td>
        <td id="file-torch101_bad_inits-py-LC5" class="blob-code blob-code-inner js-file-line">a <span class="pl-k">=</span> torch.randn(<span class="pl-c1">1</span>, <span class="pl-v">requires_grad</span><span class="pl-k">=</span><span class="pl-c1">True</span>, <span class="pl-v">dtype</span><span class="pl-k">=</span>torch.float)</td>
      </tr>
      <tr>
        <td id="file-torch101_bad_inits-py-L6" class="blob-num js-line-number" data-line-number="6"></td>
        <td id="file-torch101_bad_inits-py-LC6" class="blob-code blob-code-inner js-file-line">b <span class="pl-k">=</span> torch.randn(<span class="pl-c1">1</span>, <span class="pl-v">requires_grad</span><span class="pl-k">=</span><span class="pl-c1">True</span>, <span class="pl-v">dtype</span><span class="pl-k">=</span>torch.float)</td>
      </tr>
      <tr>
        <td id="file-torch101_bad_inits-py-L7" class="blob-num js-line-number" data-line-number="7"></td>
        <td id="file-torch101_bad_inits-py-LC7" class="blob-code blob-code-inner js-file-line"><span class="pl-c1">print</span>(a, b)</td>
      </tr>
      <tr>
        <td id="file-torch101_bad_inits-py-L8" class="blob-num js-line-number" data-line-number="8"></td>
        <td id="file-torch101_bad_inits-py-LC8" class="blob-code blob-code-inner js-file-line">
</td>
      </tr>
      <tr>
        <td id="file-torch101_bad_inits-py-L9" class="blob-num js-line-number" data-line-number="9"></td>
        <td id="file-torch101_bad_inits-py-LC9" class="blob-code blob-code-inner js-file-line"><span class="pl-c"><span class="pl-c">#</span> SECOND</span></td>
      </tr>
      <tr>
        <td id="file-torch101_bad_inits-py-L10" class="blob-num js-line-number" data-line-number="10"></td>
        <td id="file-torch101_bad_inits-py-LC10" class="blob-code blob-code-inner js-file-line"><span class="pl-c"><span class="pl-c">#</span> But what if we want to run it on a GPU? We could just send them to device, right?</span></td>
      </tr>
      <tr>
        <td id="file-torch101_bad_inits-py-L11" class="blob-num js-line-number" data-line-number="11"></td>
        <td id="file-torch101_bad_inits-py-LC11" class="blob-code blob-code-inner js-file-line">a <span class="pl-k">=</span> torch.randn(<span class="pl-c1">1</span>, <span class="pl-v">requires_grad</span><span class="pl-k">=</span><span class="pl-c1">True</span>, <span class="pl-v">dtype</span><span class="pl-k">=</span>torch.float).to(device)</td>
      </tr>
      <tr>
        <td id="file-torch101_bad_inits-py-L12" class="blob-num js-line-number" data-line-number="12"></td>
        <td id="file-torch101_bad_inits-py-LC12" class="blob-code blob-code-inner js-file-line">b <span class="pl-k">=</span> torch.randn(<span class="pl-c1">1</span>, <span class="pl-v">requires_grad</span><span class="pl-k">=</span><span class="pl-c1">True</span>, <span class="pl-v">dtype</span><span class="pl-k">=</span>torch.float).to(device)</td>
      </tr>
      <tr>
        <td id="file-torch101_bad_inits-py-L13" class="blob-num js-line-number" data-line-number="13"></td>
        <td id="file-torch101_bad_inits-py-LC13" class="blob-code blob-code-inner js-file-line"><span class="pl-c1">print</span>(a, b)</td>
      </tr>
      <tr>
        <td id="file-torch101_bad_inits-py-L14" class="blob-num js-line-number" data-line-number="14"></td>
        <td id="file-torch101_bad_inits-py-LC14" class="blob-code blob-code-inner js-file-line"><span class="pl-c"><span class="pl-c">#</span> Sorry, but NO! The to(device) "shadows" the gradient...</span></td>
      </tr>
      <tr>
        <td id="file-torch101_bad_inits-py-L15" class="blob-num js-line-number" data-line-number="15"></td>
        <td id="file-torch101_bad_inits-py-LC15" class="blob-code blob-code-inner js-file-line">
</td>
      </tr>
      <tr>
        <td id="file-torch101_bad_inits-py-L16" class="blob-num js-line-number" data-line-number="16"></td>
        <td id="file-torch101_bad_inits-py-LC16" class="blob-code blob-code-inner js-file-line"><span class="pl-c"><span class="pl-c">#</span> THIRD</span></td>
      </tr>
      <tr>
        <td id="file-torch101_bad_inits-py-L17" class="blob-num js-line-number" data-line-number="17"></td>
        <td id="file-torch101_bad_inits-py-LC17" class="blob-code blob-code-inner js-file-line"><span class="pl-c"><span class="pl-c">#</span> We can either create regular tensors and send them to the device (as we did with our data)</span></td>
      </tr>
      <tr>
        <td id="file-torch101_bad_inits-py-L18" class="blob-num js-line-number" data-line-number="18"></td>
        <td id="file-torch101_bad_inits-py-LC18" class="blob-code blob-code-inner js-file-line">a <span class="pl-k">=</span> torch.randn(<span class="pl-c1">1</span>, <span class="pl-v">dtype</span><span class="pl-k">=</span>torch.float).to(device)</td>
      </tr>
      <tr>
        <td id="file-torch101_bad_inits-py-L19" class="blob-num js-line-number" data-line-number="19"></td>
        <td id="file-torch101_bad_inits-py-LC19" class="blob-code blob-code-inner js-file-line">b <span class="pl-k">=</span> torch.randn(<span class="pl-c1">1</span>, <span class="pl-v">dtype</span><span class="pl-k">=</span>torch.float).to(device)</td>
      </tr>
      <tr>
        <td id="file-torch101_bad_inits-py-L20" class="blob-num js-line-number" data-line-number="20"></td>
        <td id="file-torch101_bad_inits-py-LC20" class="blob-code blob-code-inner js-file-line"><span class="pl-c"><span class="pl-c">#</span> and THEN set them as requiring gradients...</span></td>
      </tr>
      <tr>
        <td id="file-torch101_bad_inits-py-L21" class="blob-num js-line-number" data-line-number="21"></td>
        <td id="file-torch101_bad_inits-py-LC21" class="blob-code blob-code-inner js-file-line">a.requires_grad_()</td>
      </tr>
      <tr>
        <td id="file-torch101_bad_inits-py-L22" class="blob-num js-line-number" data-line-number="22"></td>
        <td id="file-torch101_bad_inits-py-LC22" class="blob-code blob-code-inner js-file-line">b.requires_grad_()</td>
      </tr>
      <tr>
        <td id="file-torch101_bad_inits-py-L23" class="blob-num js-line-number" data-line-number="23"></td>
        <td id="file-torch101_bad_inits-py-LC23" class="blob-code blob-code-inner js-file-line"><span class="pl-c1">print</span>(a, b)</td>
      </tr>
</tbody></table>


  </div>

  </div>
</div>

      </div>
      <div class="gist-meta">
        <a href="https://gist.github.com/dvgodoy/a9b8521aa491ddc26105c6d655d2c933/raw/e60d360ef546c9af10e511c6027d66ace83b2f84/torch101_bad_inits.py" style="float:right">view raw</a>
        <a href="https://gist.github.com/dvgodoy/a9b8521aa491ddc26105c6d655d2c933#file-torch101_bad_inits-py">torch101_bad_inits.py</a>
        hosted with ❤ by <a href="https://github.com/">GitHub</a>
      </div>
    </div>
</div>
<script>var height = -1; var delayMs = 200;function notifyResize(height) {height = height ? height : document.documentElement.offsetHeight; var resized = false; if (window.donkey && donkey.resize) {donkey.resize(height); resized = true;}if (parent && parent._resizeIframe) {var obj = {iframe: window.frameElement, height: height}; parent._resizeIframe(obj); resized = true;}if (window.location && window.location.hash === "#amp=1" && window.parent && window.parent.postMessage) {window.parent.postMessage({sentinel: "amp", type: "embed-size", height: height}, "*");}if (window.webkit && window.webkit.messageHandlers && window.webkit.messageHandlers.resize) {window.webkit.messageHandlers.resize.postMessage(height); resized = true;}return resized;}function maybeResize() {if (document.documentElement.offsetHeight != height && notifyResize()) {height = document.documentElement.offsetHeight;}delayMs = Math.min(delayMs * 2, 1000000); setTimeout(maybeResize, delayMs);}maybeResize();</script></body></html>